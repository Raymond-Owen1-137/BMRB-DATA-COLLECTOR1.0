{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP89tUqsFuBSHJTIFXpEqT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raymond-Owen1-137/BMRB-DATA-COLLECTOR1.0/blob/main/bmrb_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4di0Mo0G_JVm"
      },
      "outputs": [],
      "source": [
        "def fetch_pdb_file(pdb_id):\n",
        "    \"\"\"\n",
        "    Downloads the PDB file for the given PDB ID.\n",
        "    Returns the file content as a string or None if there's an error.\n",
        "    \"\"\"\n",
        "    url = f\"https://files.rcsb.org/download/{pdb_id}.pdb\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"‚ùå Failed to fetch PDB file for {pdb_id}\")\n",
        "            return None\n",
        "        # Light sleep to avoid spamming the server\n",
        "        time.sleep(0.5)\n",
        "        return response.text\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Error fetching PDB file for {pdb_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "def fetch_avs_file(bmrb_id):\n",
        "    \"\"\"\n",
        "    Fetches the AVS_full.txt file content for a given BMRB ID.\n",
        "    Returns the text content or None if there's an error.\n",
        "    \"\"\"\n",
        "    avs_url = f\"https://bmrb.io/ftp/pub/bmrb/entry_directories/bmr{bmrb_id}/validation/AVS_full.txt\"\n",
        "    try:\n",
        "        response = requests.get(avs_url)\n",
        "        if response.status_code != 200:\n",
        "            return None\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Error fetching AVS file for BMRB {bmrb_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "# ---------------------------------------------------------------------------------\n",
        "# Parsing Functions\n",
        "# ---------------------------------------------------------------------------------\n",
        "\n",
        "def parse_pdb_secondary_structure(pdb_content):\n",
        "    \"\"\"\n",
        "    Parses PDB content to extract secondary structure info.\n",
        "    Returns (helices, sheets) where each is a list of (chain, start_res, end_res).\n",
        "    \"\"\"\n",
        "    helices, sheets = [], []\n",
        "    if not pdb_content:\n",
        "        return helices, sheets\n",
        "\n",
        "    for line in pdb_content.splitlines():\n",
        "        if line.startswith(\"HELIX\"):\n",
        "            # Chain is at column 20, start_res is columns 22-25, end_res is 34-37\n",
        "            chain = line[19]\n",
        "            start_res = int(line[21:25].strip())\n",
        "            end_res = int(line[33:37].strip())\n",
        "            helices.append((chain, start_res, end_res))\n",
        "\n",
        "        elif line.startswith(\"SHEET\"):\n",
        "            # Chain is at column 22, start_res is columns 23-26, end_res is 34-37\n",
        "            chain = line[21]\n",
        "            start_res = int(line[22:26].strip())\n",
        "            end_res = int(line[33:37].strip())\n",
        "            sheets.append((chain, start_res, end_res))\n",
        "\n",
        "    return helices, sheets\n",
        "\n",
        "def assign_secondary_structure(res_num, chain, helices, sheets):\n",
        "    \"\"\"\n",
        "    Assigns secondary structure based on chain and residue number.\n",
        "    Returns one of: \"Helix\", \"Sheet\", or \"Coil\".\n",
        "    \"\"\"\n",
        "    # Check if the residue is in a helix\n",
        "    for helix_chain, start_res, end_res in helices:\n",
        "        if helix_chain == chain and start_res <= res_num <= end_res:\n",
        "            return \"Helix\"\n",
        "\n",
        "    # Check if the residue is in a beta sheet\n",
        "    for sheet_chain, start_res, end_res in sheets:\n",
        "        if sheet_chain == chain and start_res <= res_num <= end_res:\n",
        "            return \"Sheet\"\n",
        "\n",
        "    # Default to Coil\n",
        "    return \"Coil\"\n",
        "\n",
        "def parse_residue_data(file_content):\n",
        "    \"\"\"\n",
        "    Extracts residue chemical shifts from AVS_full.txt.\n",
        "    Returns a list of dicts, each with keys: 'Residue', 'C', 'CA', 'CB'.\n",
        "    \"\"\"\n",
        "    if not file_content:\n",
        "        return []\n",
        "\n",
        "    # Regex finds lines: (Residue) then \"Ave C Shift Values>>\" and captures the entire shift section\n",
        "    residue_blocks = re.findall(r\"(\\w\\d+).*?Ave C Shift Values>>\\s*(.*?)\\n\\n\", file_content, re.DOTALL)\n",
        "    parsed_data = []\n",
        "\n",
        "    for res_id, shift_section in residue_blocks:\n",
        "        residue_data = {\"Residue\": res_id, \"C\": \"N/A\", \"CA\": \"N/A\", \"CB\": \"N/A\"}\n",
        "\n",
        "        c_match = re.search(r\"C\\s*::\\s*([\\-\\d.]+)\\s*\\([\\d.]+\\)\", shift_section)\n",
        "        ca_match = re.search(r\"CA\\s*::\\s*([\\-\\d.]+)\\s*\\([\\d.]+\\)\", shift_section)\n",
        "        cb_match = re.search(r\"CB\\s*::\\s*([\\-\\d.]+)\\s*\\([\\d.]+\\)\", shift_section)\n",
        "\n",
        "        if c_match:\n",
        "            residue_data[\"C\"] = c_match.group(1)\n",
        "        if ca_match:\n",
        "            residue_data[\"CA\"] = ca_match.group(1)\n",
        "        if cb_match:\n",
        "            residue_data[\"CB\"] = cb_match.group(1)\n",
        "\n",
        "        parsed_data.append(residue_data)\n",
        "\n",
        "    return parsed_data\n",
        "\n",
        "# ---------------------------------------------------------------------------------\n",
        "# Main Processing Workflow\n",
        "# ---------------------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import re\n",
        "import csv\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ---------------------------------------------------------------------------------\n",
        "# Constants\n",
        "# ---------------------------------------------------------------------------------\n",
        "\n",
        "QUERY_GRID_URL = (\n",
        "    \"https://bmrb.io/search/query_grid/?data_types%5B%5D=carbon_shifts\"\n",
        "    \"&polymers%5B%5D=polypeptide%28L%29&polymer_join_type=AND\"\n",
        ")\n",
        "\n",
        "# Dictionary to convert 1-letter to 3-letter amino acid names\n",
        "AMINO_ACIDS = {\n",
        "    \"A\": \"ALA\", \"R\": \"ARG\", \"N\": \"ASN\", \"D\": \"ASP\", \"C\": \"CYS\",\n",
        "    \"E\": \"GLU\", \"Q\": \"GLN\", \"G\": \"GLY\", \"H\": \"HIS\", \"I\": \"ILE\",\n",
        "    \"L\": \"LEU\", \"K\": \"LYS\", \"M\": \"MET\", \"F\": \"PHE\", \"P\": \"PRO\",\n",
        "    \"S\": \"SER\", \"T\": \"THR\", \"W\": \"TRP\", \"Y\": \"TYR\", \"V\": \"VAL\"\n",
        "}\n",
        "\n",
        "# Create base data directory\n",
        "DATA_FOLDER = \"data\"\n",
        "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
        "\n",
        "# Output folders inside the \"data/\" subdirectory\n",
        "FOLDERS = {\n",
        "    \"Helix\": os.path.join(DATA_FOLDER, \"alpha_chemicalshift\"),\n",
        "    \"Sheet\": os.path.join(DATA_FOLDER, \"beta_chemicalshift\"),\n",
        "    \"Coil\": os.path.join(DATA_FOLDER, \"coil_chemicalshift\")\n",
        "}\n",
        "\n",
        "# Ensure output folders exist inside \"data/\"\n",
        "for folder in FOLDERS.values():\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "# Log file inside \"data/\"\n",
        "LOG_FILE = os.path.join(DATA_FOLDER, \"log.txt\")\n",
        "\n",
        "# ---------------------------------------------------------------------------------\n",
        "# Main Processing Workflow\n",
        "# ---------------------------------------------------------------------------------\n",
        "\n",
        "def process_bmrb_entries():\n",
        "    \"\"\"\n",
        "    Fetch BMRB IDs, retrieve PDB files, parse chemical shifts, and save to CSV.\n",
        "    All files will be stored inside the \"data/\" directory.\n",
        "    \"\"\"\n",
        "    print(\"üîç Fetching BMRB IDs...\")\n",
        "    bmrb_ids = fetch_bmrb_ids(n=10)\n",
        "    if not bmrb_ids:\n",
        "        print(\"No BMRB IDs found. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(\"üîç Fetching PDB IDs in parallel...\")\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        pdb_ids = list(tqdm(executor.map(fetch_pdb_id, bmrb_ids), total=len(bmrb_ids)))\n",
        "\n",
        "    for bmrb_id, pdb_id in zip(bmrb_ids, pdb_ids):\n",
        "        if not pdb_id:\n",
        "            with open(LOG_FILE, \"a\") as log:\n",
        "                log.write(f\"BMRB {bmrb_id} missing PDB entry.\\n\")\n",
        "            continue\n",
        "\n",
        "        pdb_content = fetch_pdb_file(pdb_id)\n",
        "        if not pdb_content:\n",
        "            continue\n",
        "\n",
        "        helices, sheets = parse_pdb_secondary_structure(pdb_content)\n",
        "        file_content = fetch_avs_file(bmrb_id)\n",
        "\n",
        "        if not file_content:\n",
        "            with open(LOG_FILE, \"a\") as log:\n",
        "                log.write(f\"BMRB {bmrb_id} missing AVS file.\\n\")\n",
        "            continue\n",
        "\n",
        "        residue_data = parse_residue_data(file_content)\n",
        "\n",
        "        for res in residue_data:\n",
        "            chain, res_num = parse_chain_and_resnum(res[\"Residue\"])\n",
        "            one_letter_code = res[\"Residue\"][0]\n",
        "            three_letter = AMINO_ACIDS.get(one_letter_code.upper(), \"UNK\")\n",
        "\n",
        "            sec_struct = assign_secondary_structure(res_num, chain, helices, sheets)\n",
        "            folder = FOLDERS[sec_struct]\n",
        "            sec_char = {\"Helix\": \"a\", \"Sheet\": \"b\", \"Coil\": \"c\"}[sec_struct]\n",
        "\n",
        "            # ‚úÖ Fix file path issue\n",
        "            filename = os.path.join(folder, f\"{three_letter}_{sec_struct}.csv\")\n",
        "\n",
        "            with open(filename, \"a\", newline=\"\") as csvfile:\n",
        "                writer = csv.writer(csvfile)\n",
        "                if os.path.getsize(filename) == 0:\n",
        "                    writer.writerow([\"Residue\", \"C\", \"CA\", \"CB\", \"Secondary_Structure\"])\n",
        "                writer.writerow([res[\"Residue\"], res[\"C\"], res[\"CA\"], res[\"CB\"], sec_char])\n",
        "\n",
        "\n",
        "        print(f\"‚úÖ Processed BMRB {bmrb_id} (PDB: {pdb_id})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_bmrb_entries()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define base data folder\n",
        "DATA_FOLDER = \"data\"\n",
        "\n",
        "# Define input subfolders\n",
        "FOLDERS = {\n",
        "    \"Helix\": os.path.join(DATA_FOLDER, \"alpha_chemicalshift\"),\n",
        "    \"Sheet\": os.path.join(DATA_FOLDER, \"beta_chemicalshift\"),\n",
        "    \"Coil\": os.path.join(DATA_FOLDER, \"coil_chemicalshift\")\n",
        "}\n",
        "\n",
        "# Output file for the combined dataset\n",
        "OUTPUT_FILE = os.path.join(DATA_FOLDER, \"combined_chemical_shifts.csv\")\n",
        "\n",
        "# Define column names for the dataset\n",
        "columns = [\"Residue_Type\", \"Residue_ID\", \"C\", \"CA\", \"CB\", \"Secondary_Structure\"]\n",
        "\n",
        "# List to store extracted data\n",
        "data = []\n",
        "\n",
        "def extract_data_from_csv(folder_name, secondary_structure):\n",
        "    \"\"\"\n",
        "    Reads all CSV files from a given folder, extracts chemical shift data,\n",
        "    and appends it to the global data list.\n",
        "\n",
        "    Args:\n",
        "        folder_name (str): The folder containing CSV files.\n",
        "        secondary_structure (str): The secondary structure type (Helix, Sheet, Coil).\n",
        "    \"\"\"\n",
        "    for filename in os.listdir(folder_name):\n",
        "        if filename.endswith(\".csv\"):  # Process only CSV files\n",
        "            file_path = os.path.join(folder_name, filename)\n",
        "\n",
        "            # Extract residue type from the filename (e.g., \"ALA_helix.csv\" ‚Üí \"ALA\")\n",
        "            residue_type = filename.split(\"_\")[0]\n",
        "\n",
        "            # Read the CSV file\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Ensure required columns exist\n",
        "            if {\"Residue\", \"C\", \"CA\", \"CB\"}.issubset(df.columns):\n",
        "                for _, row in df.iterrows():\n",
        "                    # Extract residue ID (e.g., \"A10\" ‚Üí Residue Type: \"A\", ID: \"10\")\n",
        "                    residue_id = row[\"Residue\"]\n",
        "\n",
        "                    # Append extracted data to list\n",
        "                    data.append([\n",
        "                        residue_type,  # Residue type (e.g., ALA, LYS)\n",
        "                        residue_id,    # Residue ID (e.g., A10)\n",
        "                        row[\"C\"],      # Carbonyl shift\n",
        "                        row[\"CA\"],     # Alpha Carbon shift\n",
        "                        row[\"CB\"],     # Beta Carbon shift\n",
        "                        secondary_structure  # Secondary Structure (Helix, Sheet, Coil)\n",
        "                    ])\n",
        "\n",
        "# Extract data from all three structure types\n",
        "for struct_type, folder in FOLDERS.items():\n",
        "    extract_data_from_csv(folder, struct_type)\n",
        "\n",
        "# Convert data list to a DataFrame\n",
        "df_combined = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# Handle missing values (replace \"N/A\" with NaN for easier processing)\n",
        "df_combined.replace(\"N/A\", pd.NA, inplace=True)\n",
        "\n",
        "# Save to CSV for further analysis\n",
        "df_combined.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "print(f\"‚úÖ Data extraction complete! Combined dataset saved as '{OUTPUT_FILE}'.\")\n"
      ],
      "metadata": {
        "id": "6WSe7CKk_VvD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}